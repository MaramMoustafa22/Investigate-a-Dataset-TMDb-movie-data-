# -*- coding: utf-8 -*-
"""project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TYIjRQOR3TfPQ2Sedw4ikSn-KgLE_lrb

# Project: Investigate a Dataset (TMDb movie data )

<a id='intro'></a>
## Introduction

> This data set contains information about 10,000 movies collected from The Movie Database (TMDb), including user ratings and revenue. 
The creator continuously had a huge intrigued in observing motion pictures. As most individuals did, the creator moreover experienced great and terrible motion pictures. But what decides in case a motion picture is considered as great or terrible? There may be a few components influencig the quality of a motion picture, as for illustration the budget, class, etc. This small venture ought to offer assistance the creator to progress his information analytics abilities and investigate a few of the victory criteria for motion pictures.


## Questions to answer
> 
Which genres are most popular from year to year? What kinds of properties are associated with movies that have high revenues?
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
pd.options.display.float_format = '{:.2f}'.format

df = pd.read_csv('tmdb-movies.csv')
df.head()

df.info(),
print('Dataframe contains {} rows and {} columns'.format(df.shape[0],df.shape[1]))

df.hist(figsize=(13,13));

df.describe()

"""--> There are plenty of columns indicating a problem. Many rows seems to contain several values, which are seperated by an "|". They need to be cleaned in the first place in order to provide a proper dataset for the intended analysis. The rows containing such values are:

- cast
- director
- keywords
- genres
- production_companies

--> Evenmore, there are columns in the dataset, which are not important to the intended analysis and therefore will be removed by the author. This includes:

- imdb_id
- homepage
- overview
- release_date
- tagline

### Dealing with 0 
--> df.info( ) revealed that most columns with important content for the analysis, such us revenue, release_year, etc. have a big amount of available data for the analysis. However, the .describe() method, as well as the histogramms, indicate that many columns, especially budget and revenue, contain many "0" in their columns. In order to provide a proper analysis, the columns "budget_adj", "revenue_adj", "budget", "revenue" and "runtime" will change its "0" values into "nan" and the respective colums will be dropped for the analysis.

### Data Cleaning

--> In this section the before mentioned "0" will be replaced with nan-Values and will be dropped. Then, the rows with several values will be seperated in single columns containing only one value. This is needed in order to perform a proper analysis. Afterwards, unwanted rows for the analysis will be dropped, before the author will perform general data cleaning action as dropping duplicated lines, correct data types, check duplicated entries in some selected columns.

--> I did that to  perform a proper analysis,correct data types, check duplicated entries in some selected columns.


-->The primary goal of Data Cleaning is to identify and remove errors and duplicate data in order to create a trustworthy dataset. This improves the quality of training data for analytics and allows for more precise decision-making.
"""

df = pd.read_csv('tmdb-movies.csv', na_values='?')

df.dropna(axis=0, inplace=True)

df_c.head()

df = df.join([df_c, df_d, df_k, df_g, df_p])
df = df.drop(['cast', 'keywords', 'director', 'genres', 'production_companies', 'imdb_id', 'homepage', 'overview', 'release_date', 'tagline'], axis=1)

df.duplicated().sum()

df['original_title'].value_counts().head()

limit_4_title = df['original_title'].value_counts().index.tolist()
limit_4_title = limit_4_title[:4]
limit_4_title

limit_4_id = df['original_title'].value_counts().index.tolist()

def check_double_title(names):
    for limit in limit_4_title:
        df_title = df[df['original_title'] == limit]
        print(df_title.iloc[:,:5])
        
        
check_double_title(limit_4_title)

"""## What kinds of properties are associated with movies that have high revenues?"""

# Creating a copy of the original DataFrame
df_revenue = df

#Performing basic statistic in order to create 4 classifications 
df_revenue['revenue_adj'].describe()

bin_edges = [2.37,10465848.09,43956661.16,131648235.91,2827123750.41]
bin_names = ['very low' ,'low' ,'medium' ,'high' ] 
df_revenue['revenue_level'] = pd.cut(df_revenue['revenue_adj'], bin_edges, labels=bin_names)

#list of revenue_levels classification is created for loop in next cell
df_revenue['revenue_level'].unique()
revenue_levels = df_revenue['revenue_level'].unique().tolist()
del revenue_levels[-1]

#nested dictionary is created with 8 attributes based on revenue_levels
movie_data = {}

for level in revenue_levels:

    grouped_data = df_revenue[df_revenue['revenue_level'] == level]

    director = grouped_data.loc[:, 'director_1': 'director_6']
    frequent_director = director.stack().value_counts().idxmax()

    cast = grouped_data.loc[:, 'cast_1': 'cast_5']
    frequent_cast = cast.stack().value_counts().idxmax()

    genres = grouped_data.loc[:, 'genres_1': 'genres_5']
    frequent_genres = genres.stack().value_counts().idxmax()

    production = grouped_data.loc[:, 'production_companies_1': 'production_companies_5']
    frequent_production = production.stack().value_counts().idxmax()

    release_year = grouped_data['release_year'].value_counts().idxmax()

    avg_duration = grouped_data['runtime'].mean()
    avg_budget = grouped_data['budget_adj'].mean()
    avg_vote = grouped_data['vote_average'].mean()
  
    movie_data[level] = {'avg_duration' : avg_duration, 
                         'frequent_director' : frequent_director, 
                         'avg_budget' : avg_budget, 
                         'avg_vote' : avg_vote,
                        'frequent_cast' : frequent_cast,
                        'frequent_genres' : frequent_genres,
                        'release_year' : release_year,
                        'frequent_production_companies' : frequent_production}

movie_classes = []
frames = []

for movie_class, d in movie_data.items():
    movie_classes.append(movie_class)
    frames.append(pd.DataFrame.from_dict(d, orient='index'))
    
properties = pd.concat(frames, keys=movie_classes)  

properties.columns = ['values']
properties

"""## Did movies with higher vote count received a better rating?"""

#Slice DataFrame to get 2 columns 'vote_count' and 'vote_average'
df_vote = df.loc[:, 'vote_count' : 'vote_average']
#To compare results only entries are considered with more than 2000 votes
df_vote_2000 = df_vote[df_vote['vote_count'] > 2000]

df_vote.hist(figsize=(10,10));

""""Did movies with a higher vote count receive a better rating?" does not imply that movies with a higher vote count have a higher vote average. Also, taking into account columns with more than 2000 vote counts has no effect on the overall impression. Furthermore, the corralation does not imply that higher vote totals equal higher vote averages."""

df_vote.corr()

df_vote_2000.corr()

"""## Conclusions

-> The TMDb movie dataset provide many information on all movies. After Data Wrangling, null values, duplicated values and 0 values were removed to provide more accurate results. Also, wrong datatypes were corrected and new columns .

-> The first research question, "Which genres are most popular from year to year?" yielded unexpected results, as the most popular genre varied greatly. To the author's surprise, only 11 times were the most frequently produced genres also voted as the best genre by users. The two values differed on all of the remaining 40 occasions.

 --> The second research question, "Did movies with a higher vote count receive a better rating?" does not imply that movies with a higher vote count have a higher vote average. Also, taking into account columns with more than 2000 vote counts has no effect on the overall impression. Furthermore, the corralation does not imply that higher vote totals equal higher vote averages.

### Limitation

Although we successfully predited the above properties on TMDb movie dataset, there are many infomation removed such as rows contained 0 values and null values. The dataset was cut by few thousand rows of movies, which would definitly affect the result. Also, every movies received different number of votes. Therefore, movies with fewer votes or higher votes would not be accurate. There should be a consistent way to collect the information in order to make it useful for analyzing the data.
"""

